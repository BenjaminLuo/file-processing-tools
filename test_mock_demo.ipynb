{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "from file_processing import Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 15/15 [00:00<00:00, 2149.09file/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m instance\u001b[39m.\u001b[39mabsolute_path\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     20\u001b[0m instance\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 21\u001b[0m dir1\u001b[39m.\u001b[39;49mgenerate_report(\u001b[39m'\u001b[39;49m\u001b[39mtemp.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, open_files\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m call \u001b[39min\u001b[39;00m mock_file\u001b[39m.\u001b[39mmock_calls:\n\u001b[0;32m     23\u001b[0m     _, kwargs \u001b[39m=\u001b[39m call[\u001b[39m1\u001b[39m], call[\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\BLUO\\Downloads\\report\\file_processing\\directory.py:194\u001b[0m, in \u001b[0;36mDirectory.generate_report\u001b[1;34m(self, report_file, include_text, filters, keywords, migrate_filters, open_files, split_metadata, char_limit)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m data:\n\u001b[0;32m    193\u001b[0m     file\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mopen_file\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 194\u001b[0m     file[\u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m file[\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m/\u001b[39m \u001b[39m1e6\u001b[39m\n\u001b[0;32m    195\u001b[0m     \u001b[39mif\u001b[39;00m migrate_filters:\n\u001b[0;32m    196\u001b[0m         file[\u001b[39m'\u001b[39m\u001b[39mmigrate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_filters(file[\u001b[39m'\u001b[39m\u001b[39mfile_path\u001b[39m\u001b[39m'\u001b[39m], migrate_filters))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'size'"
     ]
    }
   ],
   "source": [
    "with patch('file_processing.file.File', autospec=True) as mock_file:\n",
    "    dir1 = Directory('tests/resources/directory_test_files')\n",
    "    \n",
    "    # print([file.processor.__dict__ for file in dir1.get_files()])\n",
    "    instance = mock_file.return_value\n",
    "    instance.process.return_value = None\n",
    "    instance.file_path.return_value = ''\n",
    "    instance.file_name.return_value = ''\n",
    "    instance.extension.return_value = ''\n",
    "    instance.owner.return_value = ''\n",
    "    instance.size.return_value = 10000000\n",
    "    instance.modification_time.return_value = 10000000\n",
    "    instance.access_time.return_value = 10000000\n",
    "    instance.creation_time.return_value = 10000000\n",
    "    instance.parent_directory.return_value = ''\n",
    "    instance.permissions.return_value = 777 \n",
    "    instance.is_file.return_value = True\n",
    "    instance.is_symlink.return_value = False\n",
    "    instance.absolute_path.return_value = ''\n",
    "    instance.metadata.return_value = {}\n",
    "    dir1.generate_report('temp.csv', open_files=False)\n",
    "    for call in mock_file.mock_calls:\n",
    "        _, kwargs = call[1], call[2]\n",
    "        assert kwargs.get('open_file') is False, \"File was opened when it should not have been\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "from file_processing import File\n",
    "\n",
    "@patch('pytesseract.image_to_string')\n",
    "def test_ocr_processing_success(mock_tesseract):\n",
    "    image_path = os.path.normpath('tests\\\\resources\\\\test_files\\\\test_ocr_text.jpg')\n",
    "    mock_tesseract.return_value = 'Test OCR'\n",
    "    file = File(image_path, use_ocr=True)\n",
    "    result = re.sub('[^A-Za-z0-9!? ]+', '', file.metadata['ocr_text'])\n",
    "\n",
    "    assert result == 'Test OCR'\n",
    "\n",
    "    _, file_extension = os.path.splitext(file.file_name)\n",
    "\n",
    "    if file_extension != '.pdf':\n",
    "        mock_tesseract.assert_called_once_with(image_path)\n",
    "\n",
    "test_ocr_processing_success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "@patch('whisper.transcribe')\n",
    "def transcribe(mock_whisper):\n",
    "    mock_whisper.return_value = {'text': 'text'}\n",
    "    return whisper.transcribe(\n",
    "        model=whisper.load_model('base'),\n",
    "        audio='tests/resources/test_files/sample_speech.aiff',\n",
    "        fp16=False\n",
    "    )\n",
    "\n",
    "transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and thank you for your continued support. Thank you.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_processing import File\n",
    "\n",
    "File('tests/resources/test_files/sample_speech.aiff', use_transcriber=True).metadata['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "import whisper\n",
    "from file_processing import File\n",
    "\n",
    "\n",
    "def test_mock_transcription(path, transcription):\n",
    "    mocked_value = {\n",
    "        'text': transcription,\n",
    "        'language': 'en'\n",
    "    }\n",
    "    with patch('whisper.transcribe', return_value=mocked_value) as mock_transcribe:\n",
    "        audio_file = File(path, use_transcriber=True)\n",
    "        mock_transcribe.assert_called()\n",
    "        assert audio_file.metadata['text'] == transcription\n",
    "\n",
    "test_mock_transcription(\n",
    "    path='tests/resources/test_files/sample_speech.aiff',\n",
    "    transcription='mocked text'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
